{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbcafc-edd7-4d48-be47-7acf44031c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NEWS CLASSIFICATION MODEL TRAINING\n",
    "# ============================================================\n",
    "\n",
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74173b40-941f-441b-b008-d9bf75ae2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load dataset\n",
    "df = pd.read_csv(\"Global_News_Dataset.csv\")\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0feeb-7106-4597-99de-06845fe76d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Inspect missing values and datatypes\n",
    "print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0aa98-624c-47fa-8d7d-e0af65372ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Data Cleaning\n",
    "df = df.dropna(subset=[\"category\"])\n",
    "df[\"author\"] = df[\"author\"].fillna(\"unknown\")\n",
    "df[\"source_id\"] = df[\"source_id\"].fillna(df[\"source_name\"])\n",
    "df[\"source_id\"] = df[\"source_id\"].fillna(\"unknown\")\n",
    "df[\"url_to_image\"] = df[\"url_to_image\"].fillna(\"no_image\")\n",
    "\n",
    "text_columns = [\"author\", \"title\", \"description\", \"content\", \"full_content\"]\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str).str.strip().replace(\"\", \"unknown\")\n",
    "\n",
    "df[\"content_clean\"] = df[\"content\"].astype(str).str.lower()\n",
    "\n",
    "missing_total = df.isnull().sum().sum()\n",
    "print(f\"\\nMissing values after cleaning: {missing_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1c5d2-f4d1-498c-af8b-83e60a358014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train-Test Split\n",
    "X = df[\"content_clean\"]\n",
    "y = df[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining size: {X_train.shape[0]}\")\n",
    "print(f\"Testing size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb98f8-2020-4961-aa2e-d88943f880af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Build Pipeline (Vectorizer + Model)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "model = LogisticRegression(max_iter=200, n_jobs=-1)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = make_pipeline(vectorizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb12f2-0c42-47d7-869a-842c31a25f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Train the model\n",
    "print(\"\\nTraining model...\")\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3926e-7bc5-4b04-85b1-cb3ffff8802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Evaluate model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199872d-1a93-4177-b24d-df9f049bc81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 9: Confusion Matrix (Top 10 categories)\n",
    "top_classes = y_train.value_counts().nlargest(10).index\n",
    "mask = y_test.isin(top_classes)\n",
    "\n",
    "cm = confusion_matrix(y_test[mask], y_pred[mask], labels=top_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=top_classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix (Top 10 Categories)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion matrix saved as 'confusion_matrix.png'\")\n",
    "\n",
    "# Step 10: Classification Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222485ac-1759-4f6b-8b51-822f4fcb2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  LIME EXPLAINABILITY ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "# Import LIME\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LIME EXPLAINABILITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8bbed-1ec5-4708-bd60-7d573ab04053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize LIME Explainer\n",
    "class_names = pipeline.classes_\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "print(f\"\\nInitialized LIME explainer for {len(class_names)} categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14576432-6c8e-40b5-aba5-34d3f0de9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Function to explain predictions (FIXED)\n",
    "def explain_prediction(idx, num_features=10, save_html=True):\n",
    "    \"\"\"\n",
    "    Explain a specific prediction using LIME\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    idx : int\n",
    "        Index of the test sample to explain\n",
    "    num_features : int\n",
    "        Number of top words to show in explanation\n",
    "    save_html : bool\n",
    "        Whether to save HTML explanation file\n",
    "    \"\"\"\n",
    "    # Get the text and true label\n",
    "    text = X_test.iloc[idx]\n",
    "    true_label = y_test.iloc[idx]\n",
    "    predicted_label = pipeline.predict([text])[0]\n",
    "      \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPLANATION FOR TEST SAMPLE #{idx}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nTrue Category: {true_label}\")\n",
    "    print(f\"Predicted Category: {predicted_label}\")\n",
    "    print(f\"\\nArticle Text (first 300 chars):\")\n",
    "    print(f\"{text[:300]}...\")\n",
    "    \n",
    "    # Generate explanation\n",
    "    print(f\"\\nGenerating LIME explanation...\")\n",
    "    exp = explainer.explain_instance(\n",
    "        text, \n",
    "        pipeline.predict_proba, \n",
    "        num_features=num_features,\n",
    "        top_labels=3\n",
    "    )\n",
    "    \n",
    "    # Show explanation in console\n",
    "    print(f\"\\nTop {num_features} words influencing the prediction:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for label in exp.available_labels()[:3]:\n",
    "        category = class_names[label]\n",
    "        print(f\"\\n{str(category).upper()}:\")  # FIXED: Convert to string\n",
    "        word_weights = exp.as_list(label=label)\n",
    "        for word, weight in word_weights:\n",
    "            direction = \"POSITIVE\" if weight > 0 else \"NEGATIVE\"\n",
    "            print(f\"  {word:20s} | {weight:+.4f} | {direction}\")\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    probs = pipeline.predict_proba([text])[0]\n",
    "    print(f\"\\nPrediction Probabilities (Top 5):\")\n",
    "    print(\"-\" * 60)\n",
    "    top_indices = np.argsort(probs)[-5:][::-1]\n",
    "    for i in top_indices:\n",
    "        category_name = str(class_names[i])  # FIXED: Convert to string first\n",
    "        print(f\"{category_name:20s} | {probs[i]:.4f} ({probs[i]*100:.2f}%)\")\n",
    "    \n",
    "     # Save HTML visualization\n",
    "    if save_html:\n",
    "        html_file = f\"lime_explanation_sample_{idx}.html\"\n",
    "        exp.save_to_file(html_file)\n",
    "        print(f\"\\nHTML explanation saved to: {html_file}\")\n",
    "    \n",
    "    return exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8810a6-af32-4849-beb1-16db6a173509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Explain multiple predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING EXPLANATIONS FOR SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Explain 3 random test samples\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test), size=3, replace=False)\n",
    "\n",
    "explanations = []\n",
    "for idx in sample_indices:\n",
    "    exp = explain_prediction(idx, num_features=10, save_html=True)\n",
    "    explanations.append(exp)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff804d68-79c4-44db-9555-ef89f8cc06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Explain misclassifications\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLAINING MISCLASSIFICATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def explain_misclassifications(num_samples=3):\n",
    "    \"\"\"Explain predictions that were incorrect\"\"\"\n",
    "    misclassified_mask = y_test != y_pred\n",
    "    misclassified_indices = np.where(misclassified_mask)[0]\n",
    "    \n",
    "    print(f\"\\nTotal misclassifications: {len(misclassified_indices)}\")\n",
    "    print(f\"Explaining {min(num_samples, len(misclassified_indices))} examples...\")\n",
    "    \n",
    "    if len(misclassified_indices) == 0:\n",
    "        print(\"No misclassifications found!\")\n",
    "        return\n",
    "    \n",
    "    sample_size = min(num_samples, len(misclassified_indices))\n",
    "    selected = np.random.choice(misclassified_indices, size=sample_size, replace=False)\n",
    "    \n",
    "    for idx in selected:\n",
    "        explain_prediction(idx, num_features=10, save_html=True)\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "explain_misclassifications(num_samples=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004982f7-62ca-469e-b159-70ddabad4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLAINABILITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal test samples: {len(X_test)}\")\n",
    "print(f\"Correctly classified: {(y_test == y_pred).sum()}\")\n",
    "print(f\"Misclassified: {(y_test != y_pred).sum()}\")\n",
    "print(f\"\\nLIME explanations generated for sample predictions\")\n",
    "print(f\"HTML files saved in current directory\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80379df1-fe80-42f2-91f8-f43349918210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
